Title: My Open-Source Machine Learning Masters (in Casablanca, Morocco)
Date: 2016-07-29 15:23
Author: Will Wolf
Lang: en
Slug: my-open-source-machine-learning-masters-in-casablanca-morocco
Status: published
Summary: A 9-month self-curated deep-dive into select topics in machine learning and distributed computing.
Image: images/open_source.png

The Open-Source Machine Learning Masters (OSMLM) is a self-curated deep-dive into select topics in machine learning and distributed computing. Educational resources are derived from online courses ([MOOCs](https://en.wikipedia.org/wiki/Massive_open_online_course)), textbooks, predictive modeling competitions, academic research ([arXiv](https://arxiv.org/)), and the open-source software community. In machine learning, both the quantity and quality of these resources - all available for free or at a trivial cost - is truly f*cking amazing.

## Why Am I Doing This?

I want to become more of a technical expert in machine learning. I want to use this expertise to solve real-world problems that actually matter (moving the field forward on purely theoretical grounds is less interesting to me).

To this end, I see two main roads: a traditional graduate program, and the OSMLM.

### Why Not Graduate School

For me, graduate school is suboptimal for 3 key reasons:

1. **It's expensive.** Upon a quick Google search, a 2-year graduate program would cost, conservatively, $80,000 in tuition fees alone. This is a wholly nontrivial sum of money that would impact how I structure the next 10 years of my life.
2. **There are *far* more dependencies.** I have to apply. I have to get accepted. I have to find the right professor. I have to find a city suitable to my broader interests and lifestyle. This takes time.
3. **By the time I finish, the field of machine learning will look fundamentally different than it did when I started.** This is the most important point of all. The only way to remain current with the latest tools and techniques is to do just that. Given the furious and only-accelerating-faster pace at which machine learning is moving, this requires much more than just a few hours on the weekend.

### Why the OSMLM

1. **I think the higher education paradigm is changing.** Access to critical, academic knowledge is increasingly democratic: [Khan Academy](https://www.khanacademy.org/) can teach me about the Central Limit Theorem as well as any statistics professor. The ~$250,000 in tuition fees commanded by an undergraduate education at a private American university is, for some, several decades of debt and concession, and for others, prohibitive beyond comedy, reason and fantasy alike. If hard-skills are your end, online self-education is an immensely attractive, intuitive, and practical road to follow - especially in an industry as meritocratic as tech.
2. **I'm keenly aware of how productive I am in a self-teaching environment.** I'm largely self-taught in data science. Before that, it was online poker: a 5-year, $50 to $150,000 journey of instructional videos, online forums, critical discussion with other players and personal coaching - all from the comfort of my bedroom. I'm very effective at learning things online.
3. **Some of the most impactful projects I've completed professionally stemmed directly from those I'd completed personally.** I would not know how to ensemble models if not for [Kaggle](https://www.kaggle.com/). I would not know how to perform hierarchical Bayesian inference if not for [Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/). The open-source data science community continues to teach me creative ways to use data to solve challenging problems. To this end, I want to consume, consume, consume.
4. **The road to further technical expertise is a function of little more than time and effort.** I have a few years' industry experience as a Data Scientist. I can write clean code and productionize machine learning things. For me, the OSMLM is nothing more than taking all of the extra-curricular time spent learning new tools and algorithms and making it a full-time job.
5. **I'm extremely motivated.** The thought of studying machine learning all day has me smiling from ear to ear. Simply put, I f*cking love this stuff.

## How Long is the OSMLM?

9-12 months. Not forever.

## Why Morocco?

I aim to speak indistinguishably fluent French and Spanish by the time I'm 30. I'm currently 27. The Spanish box is largely [checked](https://www.youtube.com/watch?v=xqO0KW3O9uU). With 6-9 months in Francophone Morocco, the French box will be largely checked as well.

Furthermore, I've always wanted to live in a Muslim country: I grew up in a predominantly Jewish suburb of Philadelphia, and have had fantastic experiences traveling the Muslim world.

## How Will I Spend My Time?

I'll be spending my best 8-10 hours of the day working from a co-working space. I'll be taking online courses, reading textbooks, participating in machine learning competitions and publishing open-source code. I intend to post frequently to this blog.

## What Will I Learn?

I have 4 main areas of focus:

1. **"Deep Learning" with flavors of: auto-encoders, recommendation, and natural language processing.** I remain obsessed with encoding real-world entities as lists of numbers. I like applications that seek to understand people better than they understand themselves. Free-form text is everywhere (and relatively quick to process).
2. **Bayesian Inference.** Because they taught me frequentist statistics in school.
3. **Game Theory and Reinforcement Learning.** I wrote an [undergraduate thesis](https://honors.libraries.psu.edu/catalog/1947) in game theory and group dynamics and remain eager to tackle more. Reinforcement Learning seems like the hipster way to solve such problems these days.
4. **Apache Spark and Distributed Computing.** I have a bit of professional experience with Spark. As data continues to grow in size, distributed computing will move from a thing Google does to a no-duh occupational necessity.

## What Does Success Look Like?

Success has a few faces:

1. **Technical.** Have the technical expertise to lead teams focused on each of the above 4 topics (weighted towards the former 3, realistically).
2. **Personal.** Learning how I best learn. How do I structure my ideal working day? Do I prefer working alone, or indeed as part of a team? What is my optimal balance of reading, thinking, and coding?
3. **Language.** I intend to speak French like it's my mother tongue.

## What Happens Afterwards?

I'm likely headed back to the Americas, where I intend to devote myself to an impossibly awesome technology project and team for a period of several years. I'd like a technical mentor as well.

## How Can You Help?

In addition to self-study, I'd like to assist a few fascinating Moroccan technology organizations with their data problems. As such, if you know anyone in-country with even the most fleeting shared interest, please put me in touch.

## In Two Sentences

The Open-Source Machine Learning Masters in Casablanca, Morocco allows me to pursue several significant personal goals at the same time. This is my Francophone machine learning adventure.

## Update: Now Finished, Here's What I Did

#### Publications:

- [Neurally Embedded Emojis](http://willwolf.io/2017/06/19/neurally-embedded-emojis/)
- [Random Effects Neural Networks in Edward and Keras](http://willwolf.io/2017/06/15/random-effects-neural-networks/)
- [Further Exploring Common Probabilistic Models](http://willwolf.io/2017/07/06/further-exploring-common-probabilistic-models/)
- [Minimizing the Negative Log-Likelihood, in English](http://willwolf.io/2017/05/18/minimizing_the_negative_log_likelihood_in_english/)
- [Transfer Learning for Flight Delay Prediction via Variational Autoencoders](http://willwolf.io/2017/05/08/transfer-learning-flight-delay-prediction/)
- [Deriving the Softmax from First Principles](http://willwolf.io/2017/04/19/deriving-the-softmax-from-first-principles/)
- [Approximating Implicit Matrix Factorization with Shallow Neural Networks](http://willwolf.io/2017/04/07/approximating-implicit-matrix-factorization-with-shallow-neural-networks/)
- [Ordered Categorical GLMs for Product Feedback Scores](http://willwolf.io/2017/03/17/ordered-categorical-glms-for-product-feedback-scores/)
    - [Shiny app](https://willwolf.shinyapps.io/ordered-categorical-a-b-test/)
- [Intercausal Reasoning in Bayesian Networks](http://willwolf.io/2017/03/13/intercausal-reasoning-in-bayesian-networks/)
- [Bayesian Inference via Simulated Annealing](http://willwolf.io/2017/02/07/bayesian-inference-via-simulated-annealing/)
    - [Shiny app](https://willwolf.shinyapps.io/bayesian-inference-simulated-annealing/)
- [RescueTime Inference via the "Poor Man's Dirichlet"](http://willwolf.io/2017/02/03/bayesian-estimation-of-rescuetime-productivity/)
    - [Shiny app](https://willwolf.shinyapps.io/rescue-time-estimation/)
- [Generating World Flags with Sparse Auto-Encoders](http://willwolf.io/2016/12/13/generating-world-flags-with-sparse-auto-encoders/)
- [Docker and Kaggle with Ernie and Bert](http://willwolf.io/2016/11/22/docker-and-kaggle-with-ernie-and-bert/)
- [Recurrent Neural Network Gradients, and Lessons Learned Therein](http://willwolf.io/2016/10/18/recurrent-neural-network-gradients-and-lessons-learned-therein/)
- [Simulating the Colombian Peace Vote: Did the "No" Really Win?](http://willwolf.io/2016/10/12/simulating-the-colombian-peace-vote-did-the-no-really-win/)

#### Notable courses, books:
- [Statistical Rethinking: A Bayesian Course with Examples in R and Stan](http://xcelab.net/rm/statistical-rethinking/)
- [Probabilistic Graphical Models: Representation, Stanford University](https://www.coursera.org/learn/probabilistic-graphical-models/home/welcome)
- [Probabilistic Graphical Models: Inference, Stanford University](https://www.coursera.org/learn/probabilistic-graphical-models-2-inference)
- [Probabilistic Graphical Models: Learning, Stanford University](https://www.coursera.org/learn/probabilistic-graphical-models-3-learning/)
- [Practical Deep Learning For Coders, fast.ai](http://course.fast.ai/)
- [Discrete Optimization, University of Melbourne](https://www.coursera.org/learn/discrete-optimization)
- [Artificial Intelligence Nanodegree (Part 1), Udacity](https://classroom.udacity.com/courses/ud889)
- [Deep Learning, Udacity](https://www.udacity.com/course/deep-learning--ud730)
- [Apache Kafka, Udemy](https://www.udemy.com/apache-kafka-tutorial-for-beginners/)

#### Code:

Repositories I published (or contributed to) unrelated to the publications above.

- [tensorflow-models](https://github.com/cavaunpeu/tensorflow-models): Some basic models in TensorFlow
- [vanilla-neural-nets](https://github.com/cavaunpeu/vanilla-neural-nets): A straightforward and highly readable implementation of vanilla neural nets
- [dotify](https://github.com/cavaunpeu/dotify): A web application that recommends songs via "country arithmetic" and hand-rolled Implicit Matrix Factorization
    - [Heroku app](http://dotify.herokuapp.com/)
- [n-queens-sympy](https://github.com/cavaunpeu/n-queens-sympy): A simple solver for the N-Queens Problem using SymPy
- [markdown-insert-screenshot](https://github.com/cavaunpeu/markdown-insert-screenshot): A lightweight Atom plugin for saving an interactive screen capture to a relative file destination